{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful links\n",
    "# Betsy - https://stats.idre.ucla.edu/stata/seminars/regression-models-with-count-data/\n",
    "# https://stats.idre.ucla.edu/r/dae/zinb/\n",
    "# https://stats.idre.ucla.edu/stata/output/zero-inflated-negative-binomial-regression/\n",
    "# https://towardsdatascience.com/an-illustrated-guide-to-the-zero-inflated-poisson-model-b22833343057\n",
    "# https://towardsdatascience.com/negative-binomial-regression-f99031bb25b4\n",
    "\n",
    "#https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "import math\n",
    "from matplotlib.ticker import FuncFormatter #for formatting axis\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats\n",
    "from patsy import dmatrices\n",
    "from stargazer.stargazer import Stargazer, LineLocation #for model comparisons\n",
    "\n",
    "# For interactive widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# turn off top and rigt axis line in matplotlib\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "\n",
    "# change font size in matplolib\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# increasing the deafult DPI to improve resolution\n",
    "#plt.rcParams['figure.dpi']= 50 # for preview\n",
    "plt.rc(\"savefig\", dpi = 300) # for saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming convention for dataframes:\n",
    "\n",
    "* sd - Secchi disk depth\n",
    "\n",
    "* t - turbidity\n",
    "\n",
    "* tss - total suspended solids\n",
    "\n",
    "* c - chlorophyll\n",
    "\n",
    "* tcc - total cell count\n",
    "\n",
    "    * tcc_c - total cell count (composite)\n",
    "    \n",
    "    * tcc_s - total cell count (surface)\n",
    "\n",
    "* tb - total biovolume\n",
    "\n",
    "    * tb_c - total biovolume (composite)\n",
    "    \n",
    "    * tb_s - total biovolume (surface)\n",
    "\n",
    "* ccc - cyano cell count\n",
    "\n",
    "    * ccc_c - cyano cell count (composite)\n",
    "    \n",
    "    * ccc_s - cyano cell count (surface)\n",
    "\n",
    "* cb - cyano biovolume\n",
    "\n",
    "    * cb_c - cyano biovolume (composite)\n",
    "    \n",
    "    * cb_s - cyano biovolume (surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate folder with csv for each variable\n",
    "data_folder = \"../../data/processed/regression/\"\n",
    "files = os.listdir(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chlorophyll_final.csv',\n",
       " 'cyano_biovolume_composite_final.csv',\n",
       " 'cyano_biovolume_final.csv',\n",
       " 'cyano_biovolume_surface_final.csv',\n",
       " 'cyano_cell_count_composite_final.csv',\n",
       " 'cyano_cell_count_final.csv',\n",
       " 'cyano_cell_count_surface_final.csv',\n",
       " 'secchi_depth_final.csv',\n",
       " 'total_biovolume_composite_final.csv',\n",
       " 'total_biovolume_final.csv',\n",
       " 'total_biovolume_surface_final.csv',\n",
       " 'total_cell_count_composite_final.csv',\n",
       " 'total_cell_count_final.csv',\n",
       " 'total_cell_count_surface_final.csv',\n",
       " 'total_suspended_solids_final.csv',\n",
       " 'turbidity_final.csv',\n",
       " 'tweets_daily_final.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check CSV files in the folder\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to hold all the dataframes\n",
    "df = {}\n",
    "\n",
    "# Name of dataframe indices - align with the order of files\n",
    "names = [\"c\", \"cb_c\", \"cb\", \"cb_s\", \"ccc_c\", \"ccc\", \"ccc_s\", \"sd\", \"tb_c\", \n",
    "         \"tb\", \"tb_s\", \"tcc_c\", \"tcc\", \"tcc_s\", \"tss\", \"t\", \"tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "for name, file in zip(names, files):\n",
    "        df[name] = pd.read_csv(f'{data_folder}{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime object\n",
    "for file in df.keys():\n",
    "    # seaprate if statement for tweets dataframe because its date column has a different name\n",
    "    if file == \"tweets\":\n",
    "        df[file]['time'] = pd.to_datetime(df[file]['time'])\n",
    "    else:\n",
    "        df[file][\"OBJECTID_1\"] = pd.to_datetime(df[file][\"OBJECTID_1\"])\n",
    "        df[file].columns = df[file].columns.str.replace(\".\", \"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge various tweet count stats with each dataframe\n",
    "\n",
    "for file in df.keys():\n",
    "\n",
    "    # Apply following steps in all the dataframes except the one for tweet counts per day\n",
    "    if file != \"tweets\":\n",
    "    \n",
    "        # create new columns to store tweet counts data\n",
    "\n",
    "        df[file]['tweets_neg_3_both'] = np.nan\n",
    "        df[file]['tweets_neg_3_after'] = np.nan\n",
    "        df[file]['tweets_neg_7_both'] = np.nan\n",
    "        df[file]['tweets_neg_7_after'] = np.nan\n",
    "\n",
    "\n",
    "        # Iterate over each row in wq variable\n",
    "        for index, row in df[file].iterrows():\n",
    "\n",
    "            # get dates for which we have water quality measurements\n",
    "            date_wq = row['OBJECTID_1']\n",
    "\n",
    "            # get index of the row with same date in the tweets data\n",
    "            tweets_index = df[\"tweets\"]['time'].eq(date_wq).idxmax()\n",
    "\n",
    "            # get tweets info from 3 & 7 days before and after the date on which water quality was measured\n",
    "            tweets_result_3_both = df[\"tweets\"].iloc[tweets_index-3: tweets_index+4]\n",
    "            tweets_result_3_after = df[\"tweets\"].iloc[tweets_index: tweets_index+4]\n",
    "\n",
    "            tweets_result_7_both = df[\"tweets\"].iloc[tweets_index-7: tweets_index+8]\n",
    "            tweets_result_7_after = df[\"tweets\"].iloc[tweets_index: tweets_index+8]\n",
    "\n",
    "            # assign the sum of negative tweets\n",
    "            df[file].loc[index, 'tweets_neg_3_both'] = tweets_result_3_both['neg'].sum() \n",
    "            df[file].loc[index, 'tweets_neg_3_after'] = tweets_result_3_after['neg'].sum() \n",
    "            df[file].loc[index, 'tweets_neg_7_both'] = tweets_result_7_both['neg'].sum() \n",
    "            df[file].loc[index, 'tweets_neg_7_after'] = tweets_result_7_after['neg'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create functions to apply similar processes for all of our explanatory water qauality variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat histogram of tweet counts for all water quality variables to see if they are zero-inflated\n",
    "# separate functions for 3 days and 7 days rolling windows\n",
    " \n",
    "# 7 days rolling window\n",
    "def histogram_7days(variable, plot_title):\n",
    "    fig = plt.figure(figsize=(15,12))\n",
    "    i = -1\n",
    "\n",
    "    for num in range(1,10):\n",
    "\n",
    "        ax1 = fig.add_subplot(3,3,num)\n",
    "        ax1.hist(variable.iloc[:,i], color='lightskyblue', linewidth=1, edgecolor=\"white\", bins=40)\n",
    "        ax1.set(xlabel = variable.columns[i], ylabel=f'Number of observations')\n",
    "        i-=1\n",
    "\n",
    "    fig.suptitle(f\"Distribution of total tweets in 7-day rolling window for {plot_title}, n={len(variable)}\")\n",
    "    #fig.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "    plt.savefig(f\"../../output/distribution/{plot_title}_7d_histogram.png\", bbox_inches = 'tight')\n",
    "    \n",
    "# 3 days rolling window\n",
    "def histogram_3days(variable, plot_title):\n",
    "    fig = plt.figure(figsize=(15,12))\n",
    "    i = -10\n",
    "\n",
    "    for num in range(1,10):\n",
    "\n",
    "        ax1 = fig.add_subplot(3,3,num)\n",
    "        ax1.hist(variable.iloc[:,i], color='pink', linewidth=1, edgecolor=\"white\", bins=40)\n",
    "        ax1.set(xlabel = variable.columns[i], ylabel=f'Number of observations')\n",
    "        i-=1\n",
    "\n",
    "    fig.suptitle(f\"Distribution of total tweets in 3-day rolling window for {plot_title}, n={len(variable)}\")\n",
    "    #fig.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "    plt.savefig(f\"../../output/distribution/{plot_title}_3d_histogram.png\", bbox_inches = 'tight')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform poisson regresion\n",
    "def poisson(wq, x, y):\n",
    "    \n",
    "    temp = df[wq]\n",
    "    \n",
    "    # Model formula\n",
    "    formula = f\"{y} ~ {x}\"\n",
    "    \n",
    "    # Use Patsy package to carve out the x and y matrices for the ZIP model \n",
    "    Y, X = dmatrices(formula, temp, return_type='dataframe')\n",
    "    \n",
    "    # Fit the model\n",
    "    model_p = sm.GLM(Y, X, family=sm.families.Poisson()).fit() #Fit the model using maximum likelihood\n",
    "    \n",
    "    return model_p.summary()\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "# Perform negative binomial\n",
    "# following poisson results will feed into negative binomial\n",
    "def poisson_for_nb(wq, x, y):\n",
    "    \n",
    "    temp = df[wq]\n",
    "    \n",
    "    # Model formula\n",
    "    formula = f\"{y} ~ {x}\"\n",
    "    \n",
    "    # Use Patsy package to carve out the x and y matrices for the ZIP model \n",
    "    Y, X = dmatrices(formula, temp, return_type='dataframe')\n",
    "    \n",
    "    # Fit the model\n",
    "    model_p = sm.GLM(Y, X, family=sm.families.Poisson()).fit() #Fit the model using maximum likelihood\n",
    "    \n",
    "    return model_p, Y, X, temp\n",
    "\n",
    "def negative_binomial(wq, x, y):\n",
    "    \n",
    "    model_p, Y, X, temp = poisson_for_nb(wq, x, y)\n",
    "    \n",
    "    # Obtain rate vector from Poisson model\n",
    "    temp2 = temp.copy()\n",
    "    temp2['rate_lambda'] = model_p.mu\n",
    "    \n",
    "    # Auxiliary OLS regression to find α for the NB2 model\n",
    "    # Calculate the dependent variable\n",
    "    temp2['aux_ols_dependent'] = temp2.apply(lambda t: ((t[y] - t['rate_lambda'])**2 - t['rate_lambda']) / t['rate_lambda'], axis=1)\n",
    "    \n",
    "    # Using the α as dependent variable in an OLS regression model\n",
    "    ols_formula = \"\"\"aux_ols_dependent ~ rate_lambda - 1\"\"\"\n",
    "    \n",
    "    # Fitting the OLS regression model\n",
    "    model_aux_olsr = smf.ols(ols_formula, temp2).fit()\n",
    "    \n",
    "    # The model coefficient is the alpha parameter for our negative binomial\n",
    "    alpha = model_aux_olsr.params[0]\n",
    "    \n",
    "    # Get the associate t-value to determine significance\n",
    "    t_value = model_aux_olsr.tvalues[0]\n",
    "    \n",
    "    # Caluclate degrees of freedom (number of observations - regression variables - dispersion variable)\n",
    "    deg_freedom = len(temp2) - 1 - 1\n",
    "    \n",
    "    # Determine t critical value at 95% significance value\n",
    "    t_critical = scipy.stats.t.ppf(q=1-.05,df=deg_freedom)\n",
    "    \n",
    "    # If the alpha is significant, negative binomial can be a better fit than poisson\n",
    "    if (t_value > t_critical):\n",
    "        alpha_sig = \"Alpha parameter of negative binomial regression is significant\"\n",
    "    else:\n",
    "        alpha_sig = \"Alpha parameter of negative binomial regression is not significant\"\n",
    "        \n",
    "    # Fit negative binomial regression model\n",
    "    model_nb = sm.GLM(Y, X,family=sm.families.NegativeBinomial(alpha=alpha)).fit()\n",
    "    \n",
    "#     # Export model as img\n",
    "#     plt.rc('figure', figsize=(7,3))\n",
    "#     plt.text(0.01, 0.05, str(nb2_training_results.summary()),\n",
    "#     {'fontsize':22}, fontproperties =  'monospace')\n",
    "#     plt.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'../../output/regression/{wq}_{y}_summary.png')\n",
    "    \n",
    "    return alpha_sig, model_nb.summary(), model_nb\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "# Perform zero inflated poisson regression (ZIP)\n",
    "def zero_inflated_poisson(water_quality_parameter, alias, independent_var, dependent_var):\n",
    "    \n",
    "    display(alias)\n",
    "    \n",
    "    # Model formula\n",
    "    formula = f\"{dependent_var} ~ {independent_var}\"\n",
    "    \n",
    "    # Use Patsy package to carve out the x and y matrices for the ZIP model \n",
    "    y, x = dmatrices(formula, water_quality_parameter, return_type='dataframe')\n",
    "    \n",
    "    # Fit the model\n",
    "    model_zip = sm.ZeroInflatedPoisson(endog=y, exog=x, exog_infl=x, inflation='logit').fit(maxiter=500, method='bfgs') #Fit the model using maximum likelihood\n",
    "    \n",
    "    return model_zip.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run either poisson or negative binmoial using widgets\n",
    "def regression_choice (Model, wq, x, y):\n",
    "    print(f\"{Model} - {y}\")\n",
    "    if Model == \"Negative Binomial\":\n",
    "        return negative_binomial(wq, x, y)\n",
    "    elif Model == \"Poisson\":\n",
    "        return poisson(wq, x, y)\n",
    "    \n",
    "\n",
    "# Exploratory model widget - Create interactive UI to run the statistical model\n",
    "def explore_models(): #FOR NOW THIS WORKS WITH THE POISSON FUNCTION BELOW\n",
    "\n",
    "    interact_manual(regression_choice, Model = [(\"Negative Binomial\", 'Negative Binomial'), (\"Poisson\", 'Poisson')],\n",
    "                    wq = [(\"Secchi disk depth\", 'sd'), (\"Chlorophyll a\", 'c'), \n",
    "                          (\"Total suspended solids\", 'tss'),(\"Turbidity\", 't'),\n",
    "                          (\"Phytoplankton cell count\", 'tcc'), (\"Phytoplankton biovolume\", 'tb'),\n",
    "                          (\"Phytoplankton cell count (Composite)\", 'tcc_c'), (\"Phytoplankton cell count (Surface)\", 'tcc_s'), \n",
    "                          (\"Phytoplankton biovolume (Composite)\", 'tb_c'), (\"Phytoplankton biovolume (Surface)\", 'tb_s'),\n",
    "                          (\"Cyanobacteria cell count\", 'ccc'), (\"Cyanobacteria biovolume\", 'cb'),\n",
    "                          (\"Cyanobacteria cell count (Composite)\", 'ccc_c'), (\"Cyanobacteria cell count (Surface)\", 'ccc_s'),\n",
    "                          (\"Cyanobacteria biovolume (Composite)\", 'cb_c'), (\"Cyanobacteria biovolume (Surface)\", 'cb_s')],\n",
    "                x = [(\"75th percentile\", 'X75'),(\"Median\", 'X50'),\n",
    "                    (\"Mean\", 'MEAN'), (\"25th percentile\", 'X25')], \n",
    "                     \n",
    "                 y = [(\"Negative tweet counts in 3 prior days\", 'tweets_neg_3_before'),\n",
    "                      (\"Negative tweet counts in 3 following days\", 'tweets_neg_3_after'),\n",
    "                      (\"Negative tweet counts in 3 days rolling window\", 'tweets_neg_3_both'),\n",
    "                      (\"Negative tweet counts in 7 prior days\", 'tweets_neg_7_before'),\n",
    "                      (\"Negative tweet counts in 7 following days\", 'tweets_neg_7_after'),\n",
    "                      (\"Negative tweet counts in 7 days rolling window\", 'tweets_neg_7_both'),\n",
    "                      (\"---------------------------------------------\", ''),\n",
    "                      (\"Total tweet counts in 3 prior days\", 'tweets_wq_3_before'),\n",
    "                      (\"Total tweet counts in 3 following days\", 'tweets_wq_3_after'),\n",
    "                      (\"Total tweet counts in 3 days rolling window\", 'tweets_wq_3_both'),\n",
    "                      (\"Total tweet counts in 7 prior days\", 'tweets_wq_7_before'),\n",
    "                      (\"Total tweet counts in 7 following days\", 'tweets_wq_7_after'),\n",
    "                      (\"Total tweet counts in 7 days rolling window\", 'tweets_wq_7_both'),\n",
    "                      (\"---------------------------------------------\", ''),\n",
    "                      (\"Positive tweet counts in 3 prior days\", 'tweets_pos_3_before'),\n",
    "                      (\"Positive tweet counts in 3 following days\", 'tweets_pos_3_after'),\n",
    "                      (\"Positive tweet counts in 3 days rolling window\", 'tweets_pos_3_both'),\n",
    "                      (\"Positive tweet counts in 7 prior days\", 'tweets_pos_7_before'),\n",
    "                      (\"Positive tweet counts in 7 following days\", 'tweets_pos_7_after'),\n",
    "                      (\"Positive tweet counts in 7 days rolling window\", 'tweets_pos_7_both'),\n",
    "                                  ]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run negative binmoial for all possible dependent variables (negative and total sentiment for now)\n",
    "def mass_regression (wq):\n",
    "    \n",
    "    # 75th percentile\n",
    "    x = 'X75'\n",
    "    \n",
    "    y = [(\"Negative tweet counts in 3 prior days\", 'tweets_neg_3_before'),\n",
    "          (\"Negative tweet counts in 3 following days\", 'tweets_neg_3_after'),\n",
    "          (\"Negative tweet counts in 3 days rolling window\", 'tweets_neg_3_both'),\n",
    "          (\"Negative tweet counts in 7 prior days\", 'tweets_neg_7_before'),\n",
    "          (\"Negative tweet counts in 7 following days\", 'tweets_neg_7_after'),\n",
    "          (\"Negative tweet counts in 7 days rolling window\", 'tweets_neg_7_both'),\n",
    "          (\"Total tweet counts in 3 prior days\", 'tweets_wq_3_before'),\n",
    "          (\"Total tweet counts in 3 following days\", 'tweets_wq_3_after'),\n",
    "          (\"Total tweet counts in 3 days rolling window\", 'tweets_wq_3_both'),\n",
    "          (\"Total tweet counts in 7 prior days\", 'tweets_wq_7_before'),\n",
    "          (\"Total tweet counts in 7 following days\", 'tweets_wq_7_after'),\n",
    "          (\"Total tweet counts in 7 days rolling window\", 'tweets_wq_7_both') #,\n",
    "#           (\"Positive tweet counts in 3 prior days\", 'tweets_pos_3_before'),\n",
    "#           (\"Positive tweet counts in 3 following days\", 'tweets_pos_3_after'),\n",
    "#           (\"Positive tweet counts in 3 days rolling window\", 'tweets_pos_3_both'),\n",
    "#           (\"Positive tweet counts in 7 prior days\", 'tweets_pos_7_before'),\n",
    "#           (\"Positive tweet counts in 7 following days\", 'tweets_pos_7_after'),\n",
    "#           (\"Positive tweet counts in 7 days rolling window\", 'tweets_pos_7_both'),\n",
    "        ]\n",
    "    \n",
    "    y1 = negative_binomial(wq, x, 'tweets_neg_3_before')\n",
    "    y1_name = f\"Negative Binomial - {y[0][0]}\"\n",
    "    \n",
    "    y2 = negative_binomial(wq, x, 'tweets_neg_3_after')\n",
    "    y2_name = f\"Negative Binomial - {y[1][0]}\"\n",
    "    \n",
    "    y3 = negative_binomial(wq, x, 'tweets_neg_3_both')\n",
    "    y3_name = f\"Negative Binomial - {y[2][0]}\"\n",
    "    \n",
    "    y4 = negative_binomial(wq, x, 'tweets_neg_7_before')\n",
    "    y4_name = f\"Negative Binomial - {y[3][0]}\"\n",
    "    \n",
    "    y5 = negative_binomial(wq, x, 'tweets_neg_7_after')\n",
    "    y5_name = f\"Negative Binomial - {y[4][0]}\"\n",
    "    \n",
    "    y6 = negative_binomial(wq, x, 'tweets_neg_7_both')\n",
    "    y6_name = f\"Negative Binomial - {y[5][0]}\"\n",
    "    \n",
    "    y7 = negative_binomial(wq, x, 'tweets_neg_3_before')\n",
    "    y7_name = f\"Negative Binomial - {y[6][0]}\"\n",
    "    \n",
    "    y8 = negative_binomial(wq, x, 'tweets_neg_3_after')\n",
    "    y8_name = f\"Negative Binomial - {y[7][0]}\"\n",
    "    \n",
    "    y9 = negative_binomial(wq, x, 'tweets_neg_3_both')\n",
    "    y9_name = f\"Negative Binomial - {y[8][0]}\"\n",
    "    \n",
    "    y10 = negative_binomial(wq, x, 'tweets_neg_7_before')\n",
    "    y10_name = f\"Negative Binomial - {y[9][0]}\"\n",
    "    \n",
    "    y11 = negative_binomial(wq, x, 'tweets_neg_7_after')\n",
    "    y11_name = f\"Negative Binomial - {y[10][0]}\"\n",
    "    \n",
    "    y12 = negative_binomial(wq, x, 'tweets_neg_7_both')\n",
    "    y12_name = f\"Negative Binomial - {y[11][0]}\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    return display(y1_name, y1[0], y1[1]), display(y2_name, y2[0], y2[1]), display(y3_name, y3[0], y3[1]), display(y4_name, y4[0], y4[1]), display(y5_name, y5[0], y5[1]), display(y6_name, y6[0], y6[1]), display(y7_name, y7[0], y7[1]), display(y8_name, y8[0], y8[1]), display(y9_name, y9[0], y9[1]), display(y10_name, y10[0], y10[1]), display(y11_name, y11[0], y11[1]), display(y12_name, y12[0], y12[1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automate mass-run model widget - Create interactive UI to run the statistical model\n",
    "def run_models(): #FOR NOW THIS WORKS WITH THE POISSON FUNCTION BELOW\n",
    "\n",
    "    interact_manual(mass_regression, wq = [(\"Secchi disk depth\", 'sd'), (\"Chlorophyll a\", 'c'), \n",
    "                          (\"Total suspended solids\", 'tss'),(\"Turbidity\", 't'),\n",
    "                          (\"Phytoplankton cell count\", 'tcc'), (\"Phytoplankton biovolume\", 'tb'),\n",
    "                          (\"Phytoplankton cell count (Composite)\", 'tcc_c'), (\"Phytoplankton cell count (Surface)\", 'tcc_s'), \n",
    "                          (\"Phytoplankton biovolume (Composite)\", 'tb_c'), (\"Phytoplankton biovolume (Surface)\", 'tb_s'),\n",
    "                          (\"Cyanobacteria cell count\", 'ccc'), (\"Cyanobacteria biovolume\", 'cb'),\n",
    "                          (\"Cyanobacteria cell count (Composite)\", 'ccc_c'), (\"Cyanobacteria cell count (Surface)\", 'ccc_s'),\n",
    "                          (\"Cyanobacteria biovolume (Composite)\", 'cb_c'), (\"Cyanobacteria biovolume (Surface)\", 'cb_s')]) #,\n",
    "#                 x = [(\"75th percentile\", 'X75'),(\"Median\", 'X50'),\n",
    "#                     (\"Mean\", 'MEAN'), (\"25th percentile\", 'X25')], \n",
    "                     \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get p-values for each model\n",
    "def get_pvalues():\n",
    "    \n",
    "    # Create an empty dictionary to hold p-value\n",
    "    pvalues = {}\n",
    "    \n",
    "    wq = [\"c\", \"cb_c\",  \"cb_s\", \"ccc_c\", \"ccc_s\", \"sd\", \"tb_c\", \n",
    "          \"tb_s\", \"tcc_c\", \"tcc_s\", \"tss\", \"t\"]\n",
    "    \n",
    "    x = 'X75'\n",
    "    \n",
    "    y =  ['tweets_neg_3_after',\n",
    "          'tweets_neg_3_both',\n",
    "          'tweets_neg_7_after',\n",
    "          'tweets_neg_7_both',\n",
    "        ]\n",
    "    \n",
    "    # Iterate through each water quality parameter\n",
    "    for i in wq:\n",
    "        \n",
    "        # Iterate through each possible independent variable\n",
    "        for j in y:\n",
    "            \n",
    "            # Calculate pvalue using the third return object[model.fit() NOT model.fit().summary()] from negative binomial function\n",
    "            # 3 decimal places only\n",
    "            pvalues[f\"{i}_{j}\"] = negative_binomial(i, x, j)[2].pvalues[1]\n",
    "                      \n",
    "    return pvalues\n",
    "            \n",
    "# Store pvalue dictionary to a new variable \n",
    "pvalues = get_pvalues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round p-values to 3 decimal places\n",
    "for key in pvalues:\n",
    "    pvalues[key] = round(pvalues[key], 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of p-values to make a dataframe that can be exported\n",
    "sd = [pvalues['sd_tweets_neg_3_after'], pvalues['sd_tweets_neg_3_both'], \n",
    "      pvalues['sd_tweets_neg_7_after'], pvalues['sd_tweets_neg_7_both']]\n",
    "\n",
    "t  = [pvalues['t_tweets_neg_3_after'], pvalues['t_tweets_neg_3_both'], \n",
    "      pvalues['t_tweets_neg_7_after'], pvalues['t_tweets_neg_7_both']]\n",
    "\n",
    "c  = [pvalues['c_tweets_neg_3_after'], pvalues['c_tweets_neg_3_both'], \n",
    "      pvalues['c_tweets_neg_7_after'], pvalues['c_tweets_neg_7_both']]\n",
    "\n",
    "tss =[pvalues['tss_tweets_neg_3_after'], pvalues['tss_tweets_neg_3_both'], \n",
    "      pvalues['tss_tweets_neg_7_after'], pvalues['tss_tweets_neg_7_both']]\n",
    "\n",
    "tcc_c = [pvalues['tcc_c_tweets_neg_3_after'], pvalues['tcc_c_tweets_neg_3_both'], \n",
    "         pvalues['tcc_c_tweets_neg_7_after'], pvalues['tcc_c_tweets_neg_7_both']]\n",
    "\n",
    "tcc_s = [pvalues['tcc_s_tweets_neg_3_after'], pvalues['tcc_s_tweets_neg_3_both'], \n",
    "         pvalues['tcc_s_tweets_neg_7_after'], pvalues['tcc_s_tweets_neg_7_both']]\n",
    "\n",
    "tb_c =  [pvalues['tb_c_tweets_neg_3_after'], pvalues['tb_c_tweets_neg_3_both'], \n",
    "         pvalues['tb_c_tweets_neg_7_after'], pvalues['tb_c_tweets_neg_7_both']]\n",
    "\n",
    "tb_s =  [pvalues['tb_s_tweets_neg_3_after'], pvalues['tb_s_tweets_neg_3_both'], \n",
    "         pvalues['tb_s_tweets_neg_7_after'], pvalues['tb_s_tweets_neg_7_both']]\n",
    "\n",
    "ccc_c = [pvalues['ccc_c_tweets_neg_3_after'], pvalues['ccc_c_tweets_neg_3_both'], \n",
    "         pvalues['ccc_c_tweets_neg_7_after'], pvalues['ccc_c_tweets_neg_7_both']]\n",
    "\n",
    "ccc_s = [pvalues['ccc_s_tweets_neg_3_after'], pvalues['ccc_s_tweets_neg_3_both'], \n",
    "         pvalues['ccc_s_tweets_neg_7_after'], pvalues['ccc_s_tweets_neg_7_both']]\n",
    "\n",
    "cb_c =  [pvalues['cb_c_tweets_neg_3_after'], pvalues['cb_c_tweets_neg_3_both'], \n",
    "         pvalues['cb_c_tweets_neg_7_after'], pvalues['cb_c_tweets_neg_7_both']]\n",
    "\n",
    "cb_s =  [pvalues['cb_s_tweets_neg_3_after'], pvalues['cb_s_tweets_neg_3_both'], \n",
    "         pvalues['cb_s_tweets_neg_7_after'], pvalues['cb_s_tweets_neg_7_both']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of p-values from the lists above\n",
    "df_pvalues = pd.DataFrame([sd, t, c, tss, tcc_c, tcc_s, tb_c, tb_s, ccc_c, ccc_s, cb_c, cb_s],\n",
    "                          index = ['Secchi disk depth', 'Turbidity', 'Chlorophyll', 'Total suspended solids',\n",
    "                                   'Phytoplankton cell count (Composite)', \n",
    "                          'Phytoplankton cell count (Surface)', \n",
    "                          'Phytoplankton biovolume (Composite)', 'Phytoplankton biovolume (Surface)',\n",
    "                          'Cyanobacteria cell count (Composite)', \n",
    "                          'Cyanobacteria cell count (Surface)', \n",
    "                          'Cyanobacteria biovolume (Composite)', 'Cyanobacteria biovolume (Surface)'],\n",
    "                          columns = ['tweets_neg_3_after', 'tweets_neg_3_both',\n",
    "                                    'tweets_neg_7_after', 'tweets_neg_7_both']\n",
    "                         )\n",
    "\n",
    "# Export\n",
    "#df_pvalues.to_csv(\"../../output/regression/pvalues.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb(wq, x, y):\n",
    "    \n",
    "    model_p, Y, X, temp = poisson_for_nb(wq, x, y)\n",
    "    \n",
    "    # Obtain rate vector from Poisson model\n",
    "    temp2 = temp.copy()\n",
    "    temp2['rate_lambda'] = model_p.mu\n",
    "    \n",
    "    # Auxiliary OLS regression to find α for the NB2 model\n",
    "    # Calculate the dependent variable\n",
    "    temp2['aux_ols_dependent'] = temp2.apply(lambda t: ((t[y] - t['rate_lambda'])**2 - t['rate_lambda']) / t['rate_lambda'], axis=1)\n",
    "    \n",
    "    # Using the α as dependent variable in an OLS regression model\n",
    "    ols_formula = \"\"\"aux_ols_dependent ~ rate_lambda - 1\"\"\"\n",
    "    \n",
    "    # Fitting the OLS regression model\n",
    "    model_aux_olsr = smf.ols(ols_formula, temp2).fit()\n",
    "    \n",
    "    # The model coefficient is the alpha parameter for our negative binomial\n",
    "    alpha = model_aux_olsr.params[0]\n",
    "    \n",
    "    # Get the associate t-value to determine significance\n",
    "    t_value = model_aux_olsr.tvalues[0]\n",
    "    \n",
    "    # Caluclate degrees of freedom (number of observations - regression variables - dispersion variable)\n",
    "    deg_freedom = len(temp2) - 1 - 1\n",
    "    \n",
    "    # Determine t critical value at 95% significance value\n",
    "    t_critical = scipy.stats.t.ppf(q=1-.05,df=deg_freedom)\n",
    "    \n",
    "    # If the alpha is significant, negative binomial can be a better fit than poisson\n",
    "    if (t_value > t_critical):\n",
    "        alpha_sig = \"Alpha parameter of negative binomial regression is significant\"\n",
    "    else:\n",
    "        alpha_sig = \"Alpha parameter of negative binomial regression is not significant\"\n",
    "        \n",
    "    # Fit negative binomial regression model\n",
    "    model_nb = sm.GLM(Y, X,family=sm.families.NegativeBinomial(alpha=alpha)).fit()\n",
    "    \n",
    "    return model_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nb('ccc_s', 'X75', 'tweets_neg_7_after')\n",
    "b = nb('ccc_s', 'X75', 'tweets_neg_3_after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = Stargazer([a, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\begin{table}[!htbp] \\\\centering\\n\\\\begin{tabular}{@{\\\\extracolsep{5pt}}lcc}\\n\\\\\\\\[-1.8ex]\\\\hline\\n\\\\hline \\\\\\\\[-1.8ex]\\n\\\\\\\\[-1.8ex] & (1) & (2) \\\\\\\\\\n\\\\hline \\\\\\\\[-1.8ex]\\n Intercept & 2.595$^{***}$ & 2.599$^{***}$ \\\\\\\\\\n  & (0.595) & (0.502) \\\\\\\\\\n X75 & -3.916$^{*}$ & -2.402$^{}$ \\\\\\\\\\n  & (2.035) & (1.648) \\\\\\\\\\n\\\\hline \\\\\\\\[-1.8ex]\\n Observations & 38 & 38 \\\\\\\\\\n $R^2$ &  &  \\\\\\\\\\n Adjusted $R^2$ &  &  \\\\\\\\\\n Residual Std. Error & 1.000(df = 36) & 1.000(df = 36)  \\\\\\\\\\n F Statistic & $^{}$ (df = 1; 36) & $^{}$ (df = 1; 36) \\\\\\\\\\n\\\\hline\\n\\\\hline \\\\\\\\[-1.8ex]\\n\\\\textit{Note:} & \\\\multicolumn{2}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\\\\\\n\\\\end{tabular}\\n\\\\end{table}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.render_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"text-align:center\"><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Intercept</td><td>1.559<sup>***</sup></td><td>2.294<sup>***</sup></td></tr><tr><td style=\"text-align:left\"></td><td>(0.268)</td><td>(0.222)</td></tr><tr><td style=\"text-align:left\">X75</td><td>0.000<sup>***</sup></td><td>0.000<sup>***</sup></td></tr><tr><td style=\"text-align:left\"></td><td>(0.000)</td><td>(0.000)</td></tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Observations</td><td>61</td><td>53</td></tr><tr><td style=\"text-align: left\">R<sup>2</sup></td><td></td><td></td></tr><tr><td style=\"text-align: left\">Adjusted R<sup>2</sup></td><td></td><td></td></tr><tr><td style=\"text-align: left\">Residual Std. Error</td><td>1.000 (df=59)</td><td>1.000 (df=51)</td></tr><tr><td style=\"text-align: left\">F Statistic</td><td><sup></sup> (df=1; 59)</td><td><sup></sup> (df=1; 51)</td></tr><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Note:</td>\n",
       " <td colspan=\"2\" style=\"text-align: right\">\n",
       "  <sup>*</sup>p&lt;0.1;\n",
       "  <sup>**</sup>p&lt;0.05;\n",
       "  <sup>***</sup>p&lt;0.01\n",
       " </td></tr></table>"
      ],
      "text/plain": [
       "<stargazer.stargazer.Stargazer at 0x263e0125288>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3201169227365472"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>tweets_neg_7_after</td> <th>  No. Observations:  </th>  <td>    61</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>GLM</td>        <th>  Df Residuals:      </th>  <td>    59</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>     <td>NegativeBinomial</td>  <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>           <td>log</td>        <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>IRLS</td>        <th>  Log-Likelihood:    </th> <td> -184.71</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 12 Feb 2021</td>  <th>  Deviance:          </th> <td>  40.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:28:02</td>      <th>  Pearson chi2:      </th>  <td>  49.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>25</td>         <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    1.5593</td> <td>    0.268</td> <td>    5.820</td> <td> 0.000</td> <td>    1.034</td> <td>    2.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X75</th>       <td> 6.678e-08</td> <td> 1.59e-08</td> <td>    4.191</td> <td> 0.000</td> <td> 3.55e-08</td> <td>  9.8e-08</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:     tweets_neg_7_after   No. Observations:                   61\n",
       "Model:                            GLM   Df Residuals:                       59\n",
       "Model Family:        NegativeBinomial   Df Model:                            1\n",
       "Link Function:                    log   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -184.71\n",
       "Date:                Fri, 12 Feb 2021   Deviance:                       40.042\n",
       "Time:                        22:28:02   Pearson chi2:                     49.8\n",
       "No. Iterations:                    25                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      1.5593      0.268      5.820      0.000       1.034       2.084\n",
       "X75         6.678e-08   1.59e-08      4.191      0.000    3.55e-08     9.8e-08\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.84306891536803"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb('ccc_s', 'X75', 'tweets_neg_7_after').pearson_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>   <td>tweets_neg_3_after</td> <th>  No. Observations:  </th>  <td>    61</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>GLM</td>        <th>  Df Residuals:      </th>  <td>    59</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>     <td>NegativeBinomial</td>  <th>  Df Model:          </th>  <td>     1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>           <td>log</td>        <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>IRLS</td>        <th>  Log-Likelihood:    </th> <td> -143.33</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 12 Feb 2021</td>  <th>  Deviance:          </th> <td>  48.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>22:28:09</td>      <th>  Pearson chi2:      </th>  <td>  55.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>20</td>         <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>     <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    1.2054</td> <td>    0.296</td> <td>    4.077</td> <td> 0.000</td> <td>    0.626</td> <td>    1.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X75</th>       <td> 4.732e-08</td> <td> 1.75e-08</td> <td>    2.701</td> <td> 0.007</td> <td>  1.3e-08</td> <td> 8.17e-08</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:     tweets_neg_3_after   No. Observations:                   61\n",
       "Model:                            GLM   Df Residuals:                       59\n",
       "Model Family:        NegativeBinomial   Df Model:                            1\n",
       "Link Function:                    log   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -143.33\n",
       "Date:                Fri, 12 Feb 2021   Deviance:                       48.174\n",
       "Time:                        22:28:09   Pearson chi2:                     55.1\n",
       "No. Iterations:                    20                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      1.2054      0.296      4.077      0.000       0.626       1.785\n",
       "X75         4.732e-08   1.75e-08      2.701      0.007     1.3e-08    8.17e-08\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000667768707"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to hold coefficients\n",
    "\n",
    "# Name of dataframe indices - align with the order of files\n",
    "y_names = ['sd', 't', 'c', 'tss', 'tcc_c', 'tcc_s', 'tb_c', 'tb_s', \n",
    "         'ccc_c', 'ccc_s', 'cb_c', 'cb_s'] \n",
    "x_names = ['tweets_neg_3_after', 'tweets_neg_3_both',\n",
    "           'tweets_neg_7_after', 'tweets_neg_7_both']\n",
    "\n",
    "# Create a dataframe of p-values from the lists above\n",
    "df_coeff = pd.DataFrame(\n",
    "                      index = ['Secchi disk depth', 'Turbidity', 'Chlorophyll', 'Total suspended solids',\n",
    "                      'Phytoplankton cell count (Composite)', \n",
    "                      'Phytoplankton cell count (Surface)',\n",
    "                      'Phytoplankton biovolume (Composite)', 'Phytoplankton biovolume (Surface)',\n",
    "                      'Cyanobacteria cell count (Composite)', \n",
    "                      'Cyanobacteria cell count (Surface)', \n",
    "                      'Cyanobacteria biovolume (Composite)', 'Cyanobacteria biovolume (Surface)'],\n",
    "                      columns = ['tweets_neg_3_after', 'tweets_neg_3_both',\n",
    "                                'tweets_neg_7_after', 'tweets_neg_7_both']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_neg_3_after</th>\n",
       "      <th>tweets_neg_3_both</th>\n",
       "      <th>tweets_neg_7_after</th>\n",
       "      <th>tweets_neg_7_both</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Secchi disk depth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Turbidity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Chlorophyll</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total suspended solids</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Phytoplankton cell count (Composite)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Phytoplankton cell count (Surface)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Phytoplankton biovolume (Composite)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Phytoplankton biovolume (Surface)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cyanobacteria cell count (Composite)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cyanobacteria cell count (Surface)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cyanobacteria biovolume (Composite)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cyanobacteria biovolume (Surface)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     tweets_neg_3_after tweets_neg_3_both  \\\n",
       "Secchi disk depth                                   NaN               NaN   \n",
       "Turbidity                                           NaN               NaN   \n",
       "Chlorophyll                                         NaN               NaN   \n",
       "Total suspended solids                              NaN               NaN   \n",
       "Phytoplankton cell count (Composite)                NaN               NaN   \n",
       "Phytoplankton cell count (Surface)                  NaN               NaN   \n",
       "Phytoplankton biovolume (Composite)                 NaN               NaN   \n",
       "Phytoplankton biovolume (Surface)                   NaN               NaN   \n",
       "Cyanobacteria cell count (Composite)                NaN               NaN   \n",
       "Cyanobacteria cell count (Surface)                  NaN               NaN   \n",
       "Cyanobacteria biovolume (Composite)                 NaN               NaN   \n",
       "Cyanobacteria biovolume (Surface)                   NaN               NaN   \n",
       "\n",
       "                                     tweets_neg_7_after tweets_neg_7_both  \n",
       "Secchi disk depth                                   NaN               NaN  \n",
       "Turbidity                                           NaN               NaN  \n",
       "Chlorophyll                                         NaN               NaN  \n",
       "Total suspended solids                              NaN               NaN  \n",
       "Phytoplankton cell count (Composite)                NaN               NaN  \n",
       "Phytoplankton cell count (Surface)                  NaN               NaN  \n",
       "Phytoplankton biovolume (Composite)                 NaN               NaN  \n",
       "Phytoplankton biovolume (Surface)                   NaN               NaN  \n",
       "Cyanobacteria cell count (Composite)                NaN               NaN  \n",
       "Cyanobacteria cell count (Surface)                  NaN               NaN  \n",
       "Cyanobacteria biovolume (Composite)                 NaN               NaN  \n",
       "Cyanobacteria biovolume (Surface)                   NaN               NaN  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(df_coeff)):\n",
    "    for j in range(0, len(df_coeff.columns)):\n",
    "        model_coeff = nb(y_names[i], 'X75', x_names[j]).params[1]\n",
    "        \n",
    "        # exponentiate the regression coefficient to get rate ratio and get 3 decimal points\n",
    "        df_coeff.iloc[i][j] = model_coeff\n",
    "        \n",
    "        if 0.01 < df_pvalues.iloc[i][j] < 0.05:\n",
    "            df_coeff.iloc[i][j] = df_coeff.iloc[i][j].astype('str') + '*'\n",
    "        elif 0.001 < df_pvalues.iloc[i][j] < 0.01:\n",
    "            df_coeff.iloc[i][j] = df_coeff.iloc[i][j].astype('str') + '**'\n",
    "        elif df_pvalues.iloc[i][j] < 0.001:\n",
    "            df_coeff.iloc[i][j] = df_coeff.iloc[i][j].astype('str') + '***'\n",
    "\n",
    "#df_coeff.to_csv(\"../../output/model stats/rate_ratio_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_neg_3_after</th>\n",
       "      <th>tweets_neg_3_both</th>\n",
       "      <th>tweets_neg_7_after</th>\n",
       "      <th>tweets_neg_7_both</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Secchi disk depth</td>\n",
       "      <td>-4.568422129081878*</td>\n",
       "      <td>-3.314</td>\n",
       "      <td>-3.91586</td>\n",
       "      <td>-2.40207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Turbidity</td>\n",
       "      <td>0.04239565121747842***</td>\n",
       "      <td>0.03211486907388656***</td>\n",
       "      <td>0.030425684816381894***</td>\n",
       "      <td>0.02317142453639732***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Chlorophyll</td>\n",
       "      <td>0.013580690462154672**</td>\n",
       "      <td>0.0120354874586682**</td>\n",
       "      <td>0.011886399463692347**</td>\n",
       "      <td>0.009748734204271569**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Total suspended solids</td>\n",
       "      <td>0.00653418</td>\n",
       "      <td>0.000545512</td>\n",
       "      <td>0.00480272</td>\n",
       "      <td>0.00115798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Phytoplankton cell count (Composite)</td>\n",
       "      <td>-2.88908e-08</td>\n",
       "      <td>-2.07924e-08</td>\n",
       "      <td>-2.70486e-08</td>\n",
       "      <td>-1.9059e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Phytoplankton cell count (Surface)</td>\n",
       "      <td>6.92619e-08</td>\n",
       "      <td>5.362786190396325e-08*</td>\n",
       "      <td>8.875919807721691e-08***</td>\n",
       "      <td>5.991905329165405e-08**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Phytoplankton biovolume (Composite)</td>\n",
       "      <td>6.76762e-10</td>\n",
       "      <td>-1.75682e-10</td>\n",
       "      <td>4.39234e-09</td>\n",
       "      <td>3.67185e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Phytoplankton biovolume (Surface)</td>\n",
       "      <td>3.092386970772961e-10***</td>\n",
       "      <td>2.518415237135465e-10***</td>\n",
       "      <td>3.5542569081664793e-10***</td>\n",
       "      <td>3.010936312500606e-10***</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cyanobacteria cell count (Composite)</td>\n",
       "      <td>-1.50454e-08</td>\n",
       "      <td>-5.5613e-09</td>\n",
       "      <td>-1.6015e-08</td>\n",
       "      <td>-4.14893e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cyanobacteria cell count (Surface)</td>\n",
       "      <td>4.732430638349635e-08**</td>\n",
       "      <td>3.0383e-08</td>\n",
       "      <td>6.677686842416136e-08***</td>\n",
       "      <td>3.70775844005217e-08*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cyanobacteria biovolume (Composite)</td>\n",
       "      <td>-1.28502e-10</td>\n",
       "      <td>-2.00747e-09</td>\n",
       "      <td>2.78281e-09</td>\n",
       "      <td>2.64479e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Cyanobacteria biovolume (Surface)</td>\n",
       "      <td>1.76861e-10</td>\n",
       "      <td>1.36342e-10</td>\n",
       "      <td>3.4435662236189096e-10***</td>\n",
       "      <td>2.7919029544654137e-10***</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tweets_neg_3_after  \\\n",
       "Secchi disk depth                          -4.568422129081878*   \n",
       "Turbidity                               0.04239565121747842***   \n",
       "Chlorophyll                             0.013580690462154672**   \n",
       "Total suspended solids                              0.00653418   \n",
       "Phytoplankton cell count (Composite)              -2.88908e-08   \n",
       "Phytoplankton cell count (Surface)                 6.92619e-08   \n",
       "Phytoplankton biovolume (Composite)                6.76762e-10   \n",
       "Phytoplankton biovolume (Surface)     3.092386970772961e-10***   \n",
       "Cyanobacteria cell count (Composite)              -1.50454e-08   \n",
       "Cyanobacteria cell count (Surface)     4.732430638349635e-08**   \n",
       "Cyanobacteria biovolume (Composite)               -1.28502e-10   \n",
       "Cyanobacteria biovolume (Surface)                  1.76861e-10   \n",
       "\n",
       "                                             tweets_neg_3_both  \\\n",
       "Secchi disk depth                                       -3.314   \n",
       "Turbidity                               0.03211486907388656***   \n",
       "Chlorophyll                               0.0120354874586682**   \n",
       "Total suspended solids                             0.000545512   \n",
       "Phytoplankton cell count (Composite)              -2.07924e-08   \n",
       "Phytoplankton cell count (Surface)      5.362786190396325e-08*   \n",
       "Phytoplankton biovolume (Composite)               -1.75682e-10   \n",
       "Phytoplankton biovolume (Surface)     2.518415237135465e-10***   \n",
       "Cyanobacteria cell count (Composite)               -5.5613e-09   \n",
       "Cyanobacteria cell count (Surface)                  3.0383e-08   \n",
       "Cyanobacteria biovolume (Composite)               -2.00747e-09   \n",
       "Cyanobacteria biovolume (Surface)                  1.36342e-10   \n",
       "\n",
       "                                             tweets_neg_7_after  \\\n",
       "Secchi disk depth                                      -3.91586   \n",
       "Turbidity                               0.030425684816381894***   \n",
       "Chlorophyll                              0.011886399463692347**   \n",
       "Total suspended solids                               0.00480272   \n",
       "Phytoplankton cell count (Composite)               -2.70486e-08   \n",
       "Phytoplankton cell count (Surface)     8.875919807721691e-08***   \n",
       "Phytoplankton biovolume (Composite)                 4.39234e-09   \n",
       "Phytoplankton biovolume (Surface)     3.5542569081664793e-10***   \n",
       "Cyanobacteria cell count (Composite)                -1.6015e-08   \n",
       "Cyanobacteria cell count (Surface)     6.677686842416136e-08***   \n",
       "Cyanobacteria biovolume (Composite)                 2.78281e-09   \n",
       "Cyanobacteria biovolume (Surface)     3.4435662236189096e-10***   \n",
       "\n",
       "                                              tweets_neg_7_both  \n",
       "Secchi disk depth                                      -2.40207  \n",
       "Turbidity                                0.02317142453639732***  \n",
       "Chlorophyll                              0.009748734204271569**  \n",
       "Total suspended solids                               0.00115798  \n",
       "Phytoplankton cell count (Composite)                -1.9059e-08  \n",
       "Phytoplankton cell count (Surface)      5.991905329165405e-08**  \n",
       "Phytoplankton biovolume (Composite)                 3.67185e-09  \n",
       "Phytoplankton biovolume (Surface)      3.010936312500606e-10***  \n",
       "Cyanobacteria cell count (Composite)               -4.14893e-09  \n",
       "Cyanobacteria cell count (Surface)        3.70775844005217e-08*  \n",
       "Cyanobacteria biovolume (Composite)                 2.64479e-09  \n",
       "Cyanobacteria biovolume (Surface)     2.7919029544654137e-10***  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a dictionary to hold coefficients\n",
    "# coeff = {}\n",
    "# # Name of dataframe indices - align with the order of files\n",
    "# y_names = ['sd'] \n",
    "# x_names = ['tweets_neg_3_after', 'tweets_neg_3_both',\n",
    "#            'tweets_neg_7_after', 'tweets_neg_7_both']\n",
    "\n",
    "# # Create a dataframe of p-values from the lists above\n",
    "# df_stats = pd.DataFrame(\n",
    "#                       index = ['Intercept','', 'Secchi disk depth','', 'Observations', ],\n",
    "#                       columns = ['tweets_neg_3_after', 'tweets_neg_3_both',\n",
    "#                                 'tweets_neg_7_after', 'tweets_neg_7_both']\n",
    "#                      )\n",
    "\n",
    "\n",
    "# Name of dataframe indices - align with the order of files\n",
    "def full_model_summary(wq):\n",
    "    tweet_counts = ['tweets_neg_3_after', 'tweets_neg_3_both',\n",
    "                    'tweets_neg_7_after', 'tweets_neg_7_both']\n",
    "    model_x = ['Intercept', 'Secchi disk depth', 'Observations', 'Df', 'Log-likelihood', 'Deviance', 'Pearson Chi2']\n",
    "\n",
    "    #a = ['coef_3_after', 'stderr', 'z', 'P>|z|', '[0.025', '0.975]']\n",
    "\n",
    "    arrays = [\n",
    "               np.array([\"3 days after\", \"3 days after\", \"3 days after\", \"3 days after\", \n",
    "                         \"3 days before and after\", \"3 days before and after\", \"3 days before and after\", \"3 days before and after\",\n",
    "                         \"7 days after\", \"7 days after\", \"7 days after\", \"7 days after\",\n",
    "                         \"7 days before and after\", \"7 days before and after\", \"7 days before and after\", \"7 days before and after\"]),\n",
    "               np.array(['coef', 'std err', 'z', 'P>|z|',\n",
    "                         'coef', 'std err', 'z', 'P>|z|',\n",
    "                         'coef', 'std err', 'z', 'P>|z|',\n",
    "                         'coef', 'std err', 'z', 'P>|z|']),\n",
    "           ]\n",
    "\n",
    "    # Create a dataframe of p-values from the lists above\n",
    "    df_stats = pd.DataFrame( np.zeros((16,7), dtype='float'),\n",
    "                          index = arrays,\n",
    "                         )\n",
    "\n",
    "    # Rename columns\n",
    "    df_stats.columns = model_x\n",
    "\n",
    "    # Transpose\n",
    "    df_stats = df_stats.T\n",
    "    df_stats = df_stats.replace(0, '')\n",
    "\n",
    "    tweet_counts = ['tweets_neg_3_after', 'tweets_neg_3_both', 'tweets_neg_7_after', 'tweets_neg_7_both']\n",
    "    j = 0\n",
    "\n",
    "    for i in range (0, 16, 4):\n",
    "        # get coefficients\n",
    "        df_stats.iloc[0, i] = nb(wq, 'X75', tweet_counts[j]).params[0]\n",
    "        df_stats.iloc[1, i] = nb(wq, 'X75', tweet_counts[j]).params[1]\n",
    "\n",
    "        # get standard errors\n",
    "        df_stats.iloc[0, i+1] = nb(wq, 'X75', tweet_counts[j]).bse[0]\n",
    "        df_stats.iloc[1, i+1] = nb(wq, 'X75', tweet_counts[j]).bse[1]\n",
    "\n",
    "        # get z score\n",
    "        df_stats.iloc[0, i+2] = nb(wq, 'X75', tweet_counts[j]).tvalues[0]\n",
    "        df_stats.iloc[1, i+2] = nb(wq, 'X75', tweet_counts[j]).tvalues[1]\n",
    "\n",
    "        # get pvalues\n",
    "        df_stats.iloc[0, i+3] = nb(wq, 'X75', tweet_counts[j]).pvalues[0]\n",
    "        df_stats.iloc[1, i+3] = nb(wq, 'X75', tweet_counts[j]).pvalues[1]\n",
    "\n",
    "        # get number of observations\n",
    "        df_stats.iloc[2, i] = nb(wq, 'X75', tweet_counts[j]).nobs \n",
    "\n",
    "        # get degrees of freedom\n",
    "        df_stats.iloc[3,i] = nb(wq, 'X75', tweet_counts[j]).df_resid\n",
    "\n",
    "        # get log-likelihood\n",
    "        df_stats.iloc[4,i] = nb(wq, 'X75', tweet_counts[j]).llf\n",
    "\n",
    "        # get deviance\n",
    "        df_stats.iloc[5,i] = nb(wq, 'X75', tweet_counts[j]).deviance\n",
    "\n",
    "        # get Pearson Chi2\n",
    "        df_stats.iloc[6,i] = nb(wq, 'X75', tweet_counts[j]).pearson_chi2\n",
    "\n",
    "        j+=1\n",
    "    \n",
    "    # Export\n",
    "    return df_stats.to_excel(f\"../../output/model stats/{wq}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all model summary\n",
    "wq = [\"c\", \"cb_c\",  \"cb_s\", \"ccc_c\", \"ccc_s\", \"sd\", \"tb_c\", \n",
    "          \"tb_s\", \"tcc_c\", \"tcc_s\", \"tss\", \"t\"]\n",
    "\n",
    "for i in wq:\n",
    "    full_model_summary(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">3 days after</th>\n",
       "      <th colspan=\"4\" halign=\"left\">3 days before and after</th>\n",
       "      <th colspan=\"4\" halign=\"left\">7 days after</th>\n",
       "      <th colspan=\"4\" halign=\"left\">7 days before and after</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "      <th>coef</th>\n",
       "      <th>std err</th>\n",
       "      <th>z</th>\n",
       "      <th>P&gt;|z|</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Intercept</td>\n",
       "      <td>2.584244</td>\n",
       "      <td>0.642855</td>\n",
       "      <td>4.019947</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>2.531052</td>\n",
       "      <td>0.573192</td>\n",
       "      <td>4.415717</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>2.594733</td>\n",
       "      <td>0.595096</td>\n",
       "      <td>4.360188</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>2.598544</td>\n",
       "      <td>0.501867</td>\n",
       "      <td>5.177757</td>\n",
       "      <td>2.245699e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Secchi disk depth</td>\n",
       "      <td>-4.568422</td>\n",
       "      <td>2.251411</td>\n",
       "      <td>-2.029138</td>\n",
       "      <td>0.042444</td>\n",
       "      <td>-3.313996</td>\n",
       "      <td>1.923556</td>\n",
       "      <td>-1.722849</td>\n",
       "      <td>0.084916</td>\n",
       "      <td>-3.915864</td>\n",
       "      <td>2.034668</td>\n",
       "      <td>-1.924572</td>\n",
       "      <td>0.054283</td>\n",
       "      <td>-2.402065</td>\n",
       "      <td>1.647652</td>\n",
       "      <td>-1.457871</td>\n",
       "      <td>1.448760e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Observations</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Df</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Log-likelihood</td>\n",
       "      <td>-83.261132</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-95.136770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-96.302263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-112.778636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Deviance</td>\n",
       "      <td>51.438524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.907537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.810250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>38.252232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Pearson Chi2</td>\n",
       "      <td>60.096706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.716889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.460199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.024042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  3 days after                                \\\n",
       "                          coef   std err         z     P>|z|   \n",
       "Intercept             2.584244  0.642855  4.019947  0.000058   \n",
       "Secchi disk depth    -4.568422  2.251411 -2.029138  0.042444   \n",
       "Observations         38.000000  0.000000  0.000000  0.000000   \n",
       "Df                   36.000000  0.000000  0.000000  0.000000   \n",
       "Log-likelihood      -83.261132  0.000000  0.000000  0.000000   \n",
       "Deviance             51.438524  0.000000  0.000000  0.000000   \n",
       "Pearson Chi2         60.096706  0.000000  0.000000  0.000000   \n",
       "\n",
       "                  3 days before and after                                \\\n",
       "                                     coef   std err         z     P>|z|   \n",
       "Intercept                        2.531052  0.573192  4.415717  0.000010   \n",
       "Secchi disk depth               -3.313996  1.923556 -1.722849  0.084916   \n",
       "Observations                    38.000000  0.000000  0.000000  0.000000   \n",
       "Df                              36.000000  0.000000  0.000000  0.000000   \n",
       "Log-likelihood                 -95.136770  0.000000  0.000000  0.000000   \n",
       "Deviance                        48.907537  0.000000  0.000000  0.000000   \n",
       "Pearson Chi2                    45.716889  0.000000  0.000000  0.000000   \n",
       "\n",
       "                  7 days after                                \\\n",
       "                          coef   std err         z     P>|z|   \n",
       "Intercept             2.594733  0.595096  4.360188  0.000013   \n",
       "Secchi disk depth    -3.915864  2.034668 -1.924572  0.054283   \n",
       "Observations         38.000000  0.000000  0.000000  0.000000   \n",
       "Df                   36.000000  0.000000  0.000000  0.000000   \n",
       "Log-likelihood      -96.302263  0.000000  0.000000  0.000000   \n",
       "Deviance             37.810250  0.000000  0.000000  0.000000   \n",
       "Pearson Chi2         47.460199  0.000000  0.000000  0.000000   \n",
       "\n",
       "                  7 days before and after                                    \n",
       "                                     coef   std err         z         P>|z|  \n",
       "Intercept                        2.598544  0.501867  5.177757  2.245699e-07  \n",
       "Secchi disk depth               -2.402065  1.647652 -1.457871  1.448760e-01  \n",
       "Observations                    38.000000  0.000000  0.000000  0.000000e+00  \n",
       "Df                              36.000000  0.000000  0.000000  0.000000e+00  \n",
       "Log-likelihood                -112.778636  0.000000  0.000000  0.000000e+00  \n",
       "Deviance                        38.252232  0.000000  0.000000  0.000000e+00  \n",
       "Pearson Chi2                    36.024042  0.000000  0.000000  0.000000e+00  "
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_counts = ['tweets_neg_3_after', 'tweets_neg_3_both', 'tweets_neg_7_after', 'tweets_neg_7_both']\n",
    "j = 0\n",
    "\n",
    "for i in range (0, 16, 4):\n",
    "    # get coefficients\n",
    "    df_stats.iloc[i,0] = nb('sd', 'X75', tweet_counts[j]).params[0]\n",
    "    df_stats.iloc[i,1] = nb('sd', 'X75', tweet_counts[j]).params[1]\n",
    "\n",
    "    # get standard errors\n",
    "    df_stats.iloc[i+1,0] = nb('sd', 'X75', tweet_counts[j]).bse[0]\n",
    "    df_stats.iloc[i+1,1] = nb('sd', 'X75', tweet_counts[j]).bse[1]\n",
    "\n",
    "    # get z score\n",
    "    df_stats.iloc[i+2,0] = nb('sd', 'X75', tweet_counts[j]).tvalues[0]\n",
    "    df_stats.iloc[i+2,1] = nb('sd', 'X75', tweet_counts[j]).tvalues[1]\n",
    "\n",
    "    # get pvalues\n",
    "    df_stats.iloc[i+3,0] = nb('sd', 'X75', tweet_counts[j]).pvalues[0]\n",
    "    df_stats.iloc[i+3,1] = nb('sd', 'X75', tweet_counts[j]).pvalues[1]\n",
    "\n",
    "    # get number of observations\n",
    "    df_stats.iloc[i,2] = nb('sd', 'X75', tweet_counts[j]).nobs \n",
    "\n",
    "    # get degrees of freedom\n",
    "    df_stats.iloc[i,3] = nb('sd', 'X75', tweet_counts[j]).df_resid\n",
    "\n",
    "    # get log-likelihood\n",
    "    df_stats.iloc[i,4] = nb('sd', 'X75', tweet_counts[j]).llf\n",
    "\n",
    "    # get deviance\n",
    "    df_stats.iloc[i,5] = nb('sd', 'X75', tweet_counts[j]).deviance\n",
    "\n",
    "    # get Pearson Chi2\n",
    "    df_stats.iloc[i,6] = nb('sd', 'X75', tweet_counts[j]).pearson_chi2\n",
    "    \n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_counts = ['tweets_neg_3_after', 'tweets_neg_3_both', 'tweets_neg_7_after', 'tweets_neg_7_both']\n",
    "j = 0\n",
    "\n",
    "for i in range (0, 16, 4):\n",
    "    # get coefficients\n",
    "    df_stats.iloc[0, i] = nb('sd', 'X75', tweet_counts[j]).params[0]\n",
    "    df_stats.iloc[1, i] = nb('sd', 'X75', tweet_counts[j]).params[1]\n",
    "\n",
    "    # get standard errors\n",
    "    df_stats.iloc[0, i+1] = nb('sd', 'X75', tweet_counts[j]).bse[0]\n",
    "    df_stats.iloc[1, i+1] = nb('sd', 'X75', tweet_counts[j]).bse[1]\n",
    "\n",
    "    # get z score\n",
    "    df_stats.iloc[0, i+2] = nb('sd', 'X75', tweet_counts[j]).tvalues[0]\n",
    "    df_stats.iloc[1, i+2] = nb('sd', 'X75', tweet_counts[j]).tvalues[1]\n",
    "\n",
    "    # get pvalues\n",
    "    df_stats.iloc[0, i+3] = nb('sd', 'X75', tweet_counts[j]).pvalues[0]\n",
    "    df_stats.iloc[1, i+3] = nb('sd', 'X75', tweet_counts[j]).pvalues[1]\n",
    "\n",
    "    # get number of observations\n",
    "    df_stats.iloc[2, i] = nb('sd', 'X75', tweet_counts[j]).nobs \n",
    "\n",
    "    # get degrees of freedom\n",
    "    df_stats.iloc[3,i] = nb('sd', 'X75', tweet_counts[j]).df_resid\n",
    "\n",
    "    # get log-likelihood\n",
    "    df_stats.iloc[4,i] = nb('sd', 'X75', tweet_counts[j]).llf\n",
    "\n",
    "    # get deviance\n",
    "    df_stats.iloc[5,i] = nb('sd', 'X75', tweet_counts[j]).deviance\n",
    "\n",
    "    # get Pearson Chi2\n",
    "    df_stats.iloc[6,i] = nb('sd', 'X75', tweet_counts[j]).pearson_chi2\n",
    "    \n",
    "    j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweets_neg_3_after', 'tweets_neg_3_both'], dtype='object')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats.columns[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"text-align:center\"><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\"></td><tr><td style=\"text-align:left\"></td><td>(1)</td><td>(2)</td></tr><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align:left\">Intercept</td><td>1.559<sup>***</sup></td><td>2.294<sup>***</sup></td></tr><tr><td style=\"text-align:left\"></td><td>(0.268)</td><td>(0.222)</td></tr><tr><td style=\"text-align:left\">X75</td><td>0.000<sup>***</sup></td><td>0.000<sup>***</sup></td></tr><tr><td style=\"text-align:left\"></td><td>(0.000)</td><td>(0.000)</td></tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Observations</td><td>61</td><td>53</td></tr><tr><td style=\"text-align: left\">R<sup>2</sup></td><td></td><td></td></tr><tr><td style=\"text-align: left\">Adjusted R<sup>2</sup></td><td></td><td></td></tr><tr><td style=\"text-align: left\">Residual Std. Error</td><td>1.000 (df=59)</td><td>1.000 (df=51)</td></tr><tr><td style=\"text-align: left\">F Statistic</td><td><sup></sup> (df=1; 59)</td><td><sup></sup> (df=1; 51)</td></tr><tr><td colspan=\"3\" style=\"border-bottom: 1px solid black\"></td></tr><tr><td style=\"text-align: left\">Note:</td>\n",
       " <td colspan=\"2\" style=\"text-align: right\">\n",
       "  <sup>*</sup>p&lt;0.1;\n",
       "  <sup>**</sup>p&lt;0.05;\n",
       "  <sup>***</sup>p&lt;0.01\n",
       " </td></tr></table>"
      ],
      "text/plain": [
       "<stargazer.stargazer.Stargazer at 0x263e0125288>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to make bivariate plots\n",
    "\n",
    "def bivariate_plot(variable, title_name, axis_name, unit, col):\n",
    "    \n",
    "    def millions(x, pos):\n",
    "        'The two args are the value and tick position'\n",
    "        return '%1.0fM' % (x * 1e-6)\n",
    "    \n",
    "    # Make custome axis formatter object\n",
    "    formatter = FuncFormatter(millions)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "    ax1 = fig.add_subplot(2, 2, 1)\n",
    "    ax1.scatter(df[variable]['X75'], df[variable]['tweets_neg_3_after'], color=col, \n",
    "               alpha=0.5, s=100, edgecolor='black')\n",
    "    ax1.set(xlabel=f\"{axis_name} ({unit})\", ylabel='Tweet counts',\n",
    "           title=\"3 days after\")\n",
    "\n",
    "    ax2 = fig.add_subplot(2, 2, 2)\n",
    "    ax2.scatter(df[variable]['X75'], df[variable]['tweets_neg_3_both'], color=col, \n",
    "               alpha=0.5, s=100, edgecolor='black')\n",
    "    ax2.set(xlabel=f\"{axis_name} ({unit})\", ylabel='Tweet counts', \n",
    "            title=\"3 days before and after\")\n",
    "\n",
    "    ax3 = fig.add_subplot(2, 2, 3)\n",
    "    ax3.scatter(df[variable]['X75'], df[variable]['tweets_neg_7_after'], color=col, \n",
    "               alpha=0.5, s=100, edgecolor='black')\n",
    "    ax3.set(xlabel=f\"{axis_name} ({unit})\", ylabel='Tweet counts',\n",
    "           title=\"7 days after\")\n",
    "\n",
    "    ax4 = fig.add_subplot(2, 2, 4)\n",
    "    ax4.scatter(df[variable]['X75'], df[variable]['tweets_neg_7_both'], color=col, \n",
    "               alpha=0.5, s=100, edgecolor='black')\n",
    "    ax4.set(xlabel=f\"{axis_name} ({unit})\", ylabel='Tweet counts',\n",
    "           title=\"7 days before and after\")\n",
    "\n",
    "    # Display axis in millions based on the highest value\n",
    "    if np.max(df[variable]['X75']) > 10000000:\n",
    "        ax1.xaxis.set_major_formatter(formatter)\n",
    "        ax2.xaxis.set_major_formatter(formatter)\n",
    "        ax3.xaxis.set_major_formatter(formatter)\n",
    "        ax4.xaxis.set_major_formatter(formatter)\n",
    "    \n",
    "    # Rotate ticks if max is more than 1 billion units\n",
    "    if np.max(df[variable]['X75']) > 1000000000:\n",
    "        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n",
    "        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)\n",
    "        plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)\n",
    "        plt.setp(ax4.xaxis.get_majorticklabels(), rotation=45)\n",
    "    \n",
    "    fig.suptitle(f\"Rolling sum of tweets for {title_name} (n={len(df[variable])}) measurements\")\n",
    "    #fig.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.2)\n",
    "    plt.savefig(f\"../../output/bivariate plots/{variable}_plot.png\", bbox_inches = 'tight')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secchi disk depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('sd', 'Secchi disk depth', 'Secchi disk depth', 'm', 'tomato')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turbidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('t', 'turbidity', 'Turbidity', 'NTU', 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chlorophyll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('c', 'chlorophyll', 'Chlorophyll', '$ \\mu g / L$', 'deepskyblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Suspended Solids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('tss', 'total suspended solids', 'Total Suspended Solids', 'mg/L', 'yellowgreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Cell Count (composite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('tcc_c', 'phytoplankton cell count composite', 'Cell count', 'cells/mL', 'firebrick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Cell Count (surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('tcc_s', 'phytoplankton cell count surface', 'Cell count', 'cells/mL', 'darkorange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Biovolume (composite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('tb_c', 'phytoplankton biovolume composite', 'Biovolume', '$ \\mu m^3 / mL$', 'dodgerblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total biovolume (surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('tb_s', 'phytoplankton biovolume surface', 'Biovolume', '$ \\mu m^3 / mL$', 'lime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyano Cell Count (composite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('ccc_c', 'cyanobacteria cell count composite', 'Cell count', 'cells/mL', 'tomato')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyano Cell Count (surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('ccc_s', 'cyanobacteria cell count surface', 'Cell count', 'cells/mL', 'goldenrod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyano biovolume (composite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('cb_c', 'cyanobacteria biovolume composite', 'Biovolume', '$ \\mu m^3 / mL$', 'cornflowerblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyano biovolume (surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bivariate_plot('cb_s', 'cyanobacteria biovolume surface', 'Biovolume', '$ \\mu m^3 / mL$', 'lawngreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_3days(variable, plot_title):\n",
    "    fig = plt.figure(figsize=(15,12))\n",
    "    i = -10\n",
    "\n",
    "    for num in range(1,10):\n",
    "\n",
    "        ax1 = fig.add_subplot(3,3,num)\n",
    "        ax1.hist(variable.iloc[:,i], color='pink', linewidth=1, edgecolor=\"white\", bins=40)\n",
    "        ax1.set(xlabel = variable.columns[i], ylabel=f'Number of observations')\n",
    "        i-=1\n",
    "\n",
    "    fig.suptitle(f\"Distribution of total tweets in 3-day rolling window for {plot_title}, n={len(variable)}\")\n",
    "    #fig.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## For some parameters zero-inflated poisson is not working...\n",
    "## for same reason zero-inflated negative binomial wasn't working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Secchi disk depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"sd\"], \"Secchi disk depth\"));\n",
    "display(histogram_7days(df[\"sd\"], \"Secchi disk depth\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Turbidity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"t\"], \"Turbidity\"));\n",
    "display(histogram_7days(df[\"t\"], \"Turbidity\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Chlorophyll a - Corrected for pheophytin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"c\"], \"Chlorophyll\"));\n",
    "display(histogram_7days(df[\"c\"], \"Chlorophyll\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Total Suspended Solids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"tss\"], \"Total suspended solids\"));\n",
    "display(histogram_7days(df[\"tss\"], \"Total suspended solids\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Phytoplankton cell count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"tcc\"], \"Phytoplankton cell count\"));\n",
    "display(histogram_7days(df[\"tcc\"], \"Phytoplankton cell count\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Phytoplankton cell count (composite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"tcc_c\"], \"Phytoplankton cell count (composite)\"));\n",
    "display(histogram_7days(df[\"tcc_c\"], \"Phytoplankton cell count (composite)\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Phytoplankton cell count (surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"tcc_s\"], \"Phytoplankton cell count (surface)\"));\n",
    "display(histogram_7days(df[\"tcc_s\"], \"Phytoplankton cell count (surface)\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Phytoplankton biovolume "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"tb\"], \"Phytoplankton biovolume\"));\n",
    "display(histogram_7days(df[\"tb\"], \"Phytoplankton biovolume\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IX. Phytoplankton biovolume (composite) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"tb_c\"], \"Phytoplankton biovolume (composite)\"));\n",
    "display(histogram_7days(df[\"tb_c\"], \"Phytoplankton biovolume (composite)\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Phytoplankton biovolume (surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"tb_s\"], \"Phytoplankton biovolume (surface)\"));\n",
    "display(histogram_7days(df[\"tb_s\"], \"Phytoplankton biovolume (surface)\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XI. Cyanobacteria cell count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"ccc\"], \"Cyanobacteria cell count\"));\n",
    "display(histogram_7days(df[\"ccc\"], \"Cyanobacteria cell count\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XII. Cyanobacteria cell count (composite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"ccc_c\"], \"Cyanobacteria cell count (composite)\"));\n",
    "display(histogram_7days(df[\"ccc_c\"], \"Cyanobacteria cell count (composite)\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XIII. Cyanobacteria cell count (surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"ccc_s\"], \"Cyanobacteria cell count (surface)\"));\n",
    "display(histogram_7days(df[\"ccc_s\"], \"Cyanobacteria cell count (surface)\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XIV. Cyanobacteria biovolume "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"cb\"], \"Cyanobacteria biovolume\"));\n",
    "display(histogram_7days(df[\"cb\"], \"Cyanobacteria biovolume\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XV. Cyanobacteria biovolume (composite) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"cb_c\"], \"Cyanobacteria biovolume (composite)\"));\n",
    "display(histogram_7days(df[\"cb_c\"], \"Cyanobacteria biovolume (composite)\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XVI. Cyanobacteria biovolume (surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(histogram_3days(df[\"cb_s\"], \"Cyanobacteria biovolume (surface)\"));\n",
    "display(histogram_7days(df[\"cb_s\"], \"Cyanobacteria biovolume (surface)\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"c\", \"cb\", \"ccc\", \"sd\", \"tb\", \"tcc\", \"tss\", \"t\", \"tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson(wq, x, y):\n",
    "    \n",
    "    abc = df[wq]\n",
    "    \n",
    "    # Model formula\n",
    "    formula = f\"{y} ~ {x}\"\n",
    "    \n",
    "    # Use Patsy package to carve out the x and y matrices for the ZIP model \n",
    "    Y, X = dmatrices(formula, abc, return_type='dataframe')\n",
    "    \n",
    "    # Fit the model\n",
    "    model_p = sm.GLM(Y, X, family=sm.families.Poisson()).fit() #Fit the model using maximum likelihood\n",
    "    \n",
    "    return model_p.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "i = -1\n",
    "\n",
    "for num in range(1,10):\n",
    "\n",
    "    ax1 = fig.add_subplot(3,3,num)\n",
    "    ax1.hist(sd.iloc[:,i], color='lightskyblue', linewidth=1, edgecolor=\"white\")\n",
    "    ax1.set(xlabel = sd.columns[i], ylabel=f'Number of observations')\n",
    "    i-=1\n",
    "\n",
    "fig.suptitle(f\"Tweets count distribution for {Secchi disk depth}, n={len(sd)}\")\n",
    "#fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and variance (NOPE DONT DO IT)\n",
    "# Fit zero-inflated poisson regression\n",
    "# Fit negative binomial regression and Zero-inflated binomial regression (see why? in below link)\n",
    "# Create residual vs fitter plots for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of doing poisson and negative \n",
    "# https://dius.com.au/2017/08/03/using-statsmodels-glms-to-model-beverage-consumption/\n",
    "\n",
    "# we might not always need zero inflated model\n",
    "# https://statisticalhorizons.com/zero-inflated-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['sd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import explanatory and response variables data\n",
    "tweets = pd.read_csv(\"../../data/processed/regression/tweets_daily_final.csv\")\n",
    "sd = pd.read_csv(\"../../data/processed/regression/secchi_depth_final.csv\")\n",
    "t = pd.read_csv(\"../../data/processed/regression/turbidity_final.csv\")\n",
    "tss = pd.read_csv(\"../../data/processed/regression/total_suspended_solids_final.csv\")\n",
    "c = pd.read_csv(\"../../data/processed/regression/chlorophyll_final.csv\")\n",
    "tcc = pd.read_csv(\"../../data/processed/regression/total_cell_count_final.csv\")\n",
    "tb = pd.read_csv(\"../../data/processed/regression/total_biovolume_final.csv\")\n",
    "ccc = pd.read_csv(\"../../data/processed/regression/cyano_cell_count_final.csv\")\n",
    "cb = pd.read_csv(\"../../data/processed/regression/cyano_biovolume_final.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date column to datetime object\n",
    "tweets['time'] = pd.to_datetime(tweets['time'])\n",
    "sd['OBJECTID_1'] = pd.to_datetime(sd['OBJECTID_1'])\n",
    "t['OBJECTID_1'] = pd.to_datetime(t['OBJECTID_1'])\n",
    "tss['OBJECTID_1'] = pd.to_datetime(tss['OBJECTID_1'])\n",
    "c['OBJECTID_1'] = pd.to_datetime(c['OBJECTID_1'])\n",
    "tcc['OBJECTID_1'] = pd.to_datetime(tcc['OBJECTID_1'])\n",
    "tb['OBJECTID_1'] = pd.to_datetime(tb['OBJECTID_1'])\n",
    "ccc['OBJECTID_1'] = pd.to_datetime(ccc['OBJECTID_1'])\n",
    "cb['OBJECTID_1'] = pd.to_datetime(cb['OBJECTID_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd['OBJECTID_1'] = pd.to_datetime(sd['OBJECTID_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd['OBJECTID_1'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tweets['time'].eq(sd['OBJECTID_1'][0]).idxmax()\n",
    "res = tweets.iloc[idx-7: idx+1]\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new columns to store tweet counts data\n",
    "\n",
    "sd['tweets_neg_3_both'] = np.nan\n",
    "sd['tweets_pos_3_both'] = np.nan\n",
    "sd['tweets_wq_3_both']  = np.nan \n",
    "\n",
    "sd['tweets_neg_3_before'] = np.nan\n",
    "sd['tweets_pos_3_before'] = np.nan\n",
    "sd['tweets_wq_3_before']  = np.nan \n",
    "\n",
    "sd['tweets_neg_3_after'] = np.nan\n",
    "sd['tweets_pos_3_after'] = np.nan\n",
    "sd['tweets_wq_3_after']  = np.nan\n",
    "\n",
    "\n",
    "sd['tweets_neg_7_both'] = np.nan\n",
    "sd['tweets_pos_7_both'] = np.nan\n",
    "sd['tweets_wq_7_both']  = np.nan\n",
    "\n",
    "sd['tweets_neg_7_before'] = np.nan\n",
    "sd['tweets_pos_7_before'] = np.nan\n",
    "sd['tweets_wq_7_before']  = np.nan\n",
    "\n",
    "sd['tweets_neg_7_after'] = np.nan\n",
    "sd['tweets_pos_7_after'] = np.nan\n",
    "sd['tweets_wq_7_after']  = np.nan\n",
    "\n",
    "\n",
    "# Iterate over each row in wq variable\n",
    "for index, row in sd.iterrows():\n",
    "    \n",
    "    # get dates for which we have water quality measurements\n",
    "    date_wq = row['OBJECTID_1']\n",
    "    \n",
    "    # get index of the row with same date in the tweets data\n",
    "    tweets_index = tweets['time'].eq(date_wq).idxmax()\n",
    "    \n",
    "    # get tweets info from 3 & 7 days before and after the date on which water quality was measured\n",
    "    tweets_result_3_both = tweets.iloc[tweets_index-3: tweets_index+4]\n",
    "    tweets_result_3_before = tweets.iloc[tweets_index-3: tweets_index+1]\n",
    "    tweets_result_3_after = tweets.iloc[tweets_index: tweets_index+4]\n",
    "    \n",
    "    tweets_result_7_both = tweets.iloc[tweets_index-7: tweets_index+8]\n",
    "    tweets_result_7_before = tweets.iloc[tweets_index-7: tweets_index+1]\n",
    "    tweets_result_7_after = tweets.iloc[tweets_index: tweets_index+8]\n",
    "    \n",
    "    # assign the sum of negative tweets\n",
    "    sd.loc[index, 'tweets_neg_3_both'] = tweets_result_3_both['neg'].sum() \n",
    "    sd.loc[index, 'tweets_pos_3_both'] = tweets_result_3_both['pos'].sum()\n",
    "    sd.loc[index, 'tweets_wq_3_both'] = tweets_result_3_both['water quality?'].sum()\n",
    "    \n",
    "    sd.loc[index, 'tweets_neg_3_before'] = tweets_result_3_before['neg'].sum() \n",
    "    sd.loc[index, 'tweets_pos_3_before'] = tweets_result_3_before['pos'].sum()\n",
    "    sd.loc[index, 'tweets_wq_3_before'] = tweets_result_3_before['water quality?'].sum()\n",
    "    \n",
    "    sd.loc[index, 'tweets_neg_3_after'] = tweets_result_3_after['neg'].sum() \n",
    "    sd.loc[index, 'tweets_pos_3_after'] = tweets_result_3_after['pos'].sum()\n",
    "    sd.loc[index, 'tweets_wq_3_after'] = tweets_result_3_after['water quality?'].sum()\n",
    "    \n",
    "    sd.loc[index, 'tweets_neg_7_both'] = tweets_result_7_both['neg'].sum() \n",
    "    sd.loc[index, 'tweets_pos_7_both'] = tweets_result_7_both['pos'].sum()\n",
    "    sd.loc[index, 'tweets_wq_7_both'] = tweets_result_7_both['water quality?'].sum()\n",
    "    \n",
    "    sd.loc[index, 'tweets_neg_7_before'] = tweets_result_7_before['neg'].sum() \n",
    "    sd.loc[index, 'tweets_pos_7_before'] = tweets_result_7_before['pos'].sum()\n",
    "    sd.loc[index, 'tweets_wq_7_before'] = tweets_result_7_before['water quality?'].sum()\n",
    "    \n",
    "    sd.loc[index, 'tweets_neg_7_after'] = tweets_result_7_after['neg'].sum() \n",
    "    sd.loc[index, 'tweets_pos_7_after'] = tweets_result_7_after['pos'].sum()\n",
    "    sd.loc[index, 'tweets_wq_7_after'] = tweets_result_7_after['water quality?'].sum()\n",
    "    \n",
    "    \n",
    "#     print(date_wq)\n",
    "#     print(tweets_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.columns = sd.columns.str.replace(\".\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression - Zero inflated poisson regression (zip)\n",
    "formula = \"tweets_neg_3_both ~ X75\"\n",
    "\n",
    "# Use Patsy package to carve out the X and y matrices for ZIP model \n",
    "y, x = dmatrices(formula, sd, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/an-illustrated-guide-to-the-zero-inflated-poisson-model-b22833343057\n",
    "model_zip = sm.ZeroInflatedPoisson(endog=y, exog=x, exog_infl=x, inflation='logit').fit(method='newton') #Fit the model using maximum likelihood\n",
    "model_zip.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Regression - Zero Inflated Negative Binomial\n",
    "# explantion abt zero inflation https://stats.stackexchange.com/questions/77745/zero-inflated-negative-binomial/257900\n",
    "model_zinb = sm.ZeroInflatedNegativeBinomialP(endog=y, exog=x, exog_infl=x, inflation='logit').fit(method='ncg')\n",
    "model_zinb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Binomial\n",
    "model_p = sm.GLM(y, x, family=sm.families.Poisson()).fit()\n",
    "model_p.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_p.mu)\n",
    "print(len(model_p.mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_copy = sd.copy()\n",
    "sd_copy['BB_LAMBDA'] = model_p.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_copy['AUX_OLS_DEP'] = sd_copy.apply(lambda x: ((x['tweets_neg_3_both'] - x['BB_LAMBDA'])**2 - x['BB_LAMBDA']) / x['BB_LAMBDA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_expr = \"\"\"AUX_OLS_DEP ~ BB_LAMBDA - 1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_olsr_results = smf.ols(ols_expr, sd_copy).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aux_olsr_results.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_olsr_results.tvalues #seeing the significant t-value for 95 percentile it is significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb2_training_results = sm.GLM(y, x,family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nb2_training_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out\n",
    "# Export model as img\n",
    "plt.rc('figure', figsize=(7,3))\n",
    "plt.text(0.01, 0.05, str(nb2_training_results.summary()),\n",
    "{'fontsize':22}, fontproperties =  'monospace')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../output/regression/output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wq='sd'\n",
    "x='X75'\n",
    "y='tweets_neg_3_both'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df[wq]\n",
    "formula = f\"{y} ~ {x}\"\n",
    "Y, X = dmatrices(formula, temp, return_type='dataframe')\n",
    "model_p = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n",
    "temp2 = temp.copy()\n",
    "temp2['rate_lambda'] = model_p.mu\n",
    "temp2['aux_ols_dependent'] = temp2.apply(lambda t: ((t[y] - t['rate_lambda'])**2 - t['rate_lambda']) / t['rate_lambda'], axis=1)\n",
    "ols_formula = \"\"\"aux_ols_dependent ~ rate_lambda - 1\"\"\"\n",
    "\n",
    "model_aux_olsr = smf.ols(ols_formula, temp2).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson(wq, x, y):\n",
    "    \n",
    "    temp = df[wq]\n",
    "    \n",
    "    # Model formula\n",
    "    formula = f\"{y} ~ {x}\"\n",
    "    \n",
    "    # Use Patsy package to carve out the x and y matrices for the ZIP model \n",
    "    Y, X = dmatrices(formula, temp, return_type='dataframe')\n",
    "    \n",
    "    # Fit the model\n",
    "    model_p = sm.GLM(Y, X, family=sm.families.Poisson()).fit() #Fit the model using maximum likelihood\n",
    "    \n",
    "    return model_p, Y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_binomial(wq, x, y):\n",
    "    \n",
    "    poisson(wq, x, y)\n",
    "    \n",
    "#     temp = df[wq]\n",
    "    \n",
    "#     # Model formula\n",
    "#     formula = f\"{y} ~ {x}\"\n",
    "    \n",
    "#     # Use Patsy package to carve out the x and y matrices for the ZIP model \n",
    "#     Y, X = dmatrices(formula, temp, return_type='dataframe')\n",
    "    \n",
    "#     # Fit the model\n",
    "#     model_p = sm.GLM(Y, X, family=sm.families.Poisson()).fit() #Fit the model using maximum likelihood\n",
    "    \n",
    "    # Obtain rate vector from Poisson model\n",
    "    temp2 = temp.copy()\n",
    "    temp2['rate_lambda'] = model_p.mu\n",
    "    \n",
    "    # Auxiliary OLS regression to find α for the NB2 model\n",
    "    # Calculate the dependent variable\n",
    "    temp2['aux_ols_dependent'] = temp2.apply(lambda t: ((t[y] - t['rate_lambda'])**2 - t['rate_lambda']) / t['rate_lambda'], axis=1)\n",
    "    \n",
    "    # Using the α as dependent variable in an OLS regression model\n",
    "    ols_formula = \"\"\"aux_ols_dependent ~ rate_lambda - 1\"\"\"\n",
    "    \n",
    "    # Fitting the OLS regression model\n",
    "    model_aux_olsr = smf.ols(ols_formula, temp2).fit()\n",
    "    \n",
    "    # The model coefficient is the alpha parameter for our negative binomial\n",
    "    alpha = model_aux_olsr.params[0]\n",
    "    \n",
    "    # Get the associate t-value to determine significance\n",
    "    t_value = model_aux_olsr.tvalues[0]\n",
    "    \n",
    "    # Caluclate degrees of freedom (number of observations - regression variables - dispersion variable)\n",
    "    deg_freedom = len(temp2) - 1 - 1\n",
    "    \n",
    "    # Determine t critical value at 95% significance value\n",
    "    t_critical = scipy.stats.t.ppf(q=1-.05,df=deg_freedom)\n",
    "    \n",
    "    # If the alpha is significant, negative binomial can be a better fit than poisson\n",
    "    if (t_value > t_critical):\n",
    "        print (\"Alpha parameter of negative binomial regression is significant\")\n",
    "    else:\n",
    "        print (\"Alpha parameter of negative binomial regression is not significant\")\n",
    "        \n",
    "    # Fit negative binomial regression model\n",
    "    model_nb = sm.GLM(Y, X,family=sm.families.NegativeBinomial(alpha=alpha)).fit()\n",
    "    \n",
    "    return model_nb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_binomial('sd', 'X75', 'tweets_neg_3_both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x\n",
    "interact(f, x=[('oneeeeeeeeeeeeeeeeeeeeffffffffffddddddddddddddddddddddddd', '10'), ('two', 20)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,11))\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 1)\n",
    "ax1.scatter(df['sd']['X75'], df['sd']['tweets_neg_3_after'], color='orange', \n",
    "           alpha=0.5, s=100, edgecolor='black')\n",
    "ax1.set(xlabel=\"Secchi disk depth (m)\", ylabel='Tweet counts',\n",
    "       title=\"3 days after\")\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 2)\n",
    "ax1.scatter(df['sd']['X75'], df['sd']['tweets_neg_3_both'], color='red', \n",
    "           alpha=0.5, s=100, edgecolor='black')\n",
    "ax1.set(xlabel=\"Secchi disk depth (m)\", ylabel='Tweet counts', \n",
    "        title=\"3 days before and after\")\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 3)\n",
    "ax1.scatter(df['sd']['X75'], df['sd']['tweets_neg_7_after'], color='cyan', \n",
    "           alpha=0.5, s=100, edgecolor='black')\n",
    "ax1.set(xlabel=\"Secchi disk depth (m)\", ylabel='Tweet counts',\n",
    "       title=\"7 days after\")\n",
    "\n",
    "ax1 = fig.add_subplot(2, 2, 4)\n",
    "ax1.scatter(df['sd']['X75'], df['sd']['tweets_neg_7_both'], color='pink', \n",
    "           alpha=0.5, s=100, edgecolor='black')\n",
    "ax1.set(xlabel=\"Secchi disk depth (m)\", ylabel='Tweet counts',\n",
    "       title=\"7 days before and after\")\n",
    "\n",
    "\n",
    "fig.suptitle(f\"Rolling sum of tweets for Secchi disk depth (n={len(df['sd'])}) measurments\")\n",
    "#fig.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.2)\n",
    "plt.savefig(\"../../output/bivariate plots/sd_plot.png\", bbox_inches = 'tight')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
